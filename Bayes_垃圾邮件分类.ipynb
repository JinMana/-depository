{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取数据集的每个目录\n",
    "path = os.getcwd()\n",
    "normal_path = os.path.join(path, 'data\\\\normal')\n",
    "spam_path = os.path.join(path, 'data\\\\spam')\n",
    "test_path = os.path.join(path, 'data\\\\test')\n",
    "\n",
    "#获取目录中的每个文件\n",
    "normal_dir = os.listdir(normal_path)\n",
    "spam_dir = os.listdir(spam_path)\n",
    "test_dir = os.listdir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词处理\n",
    "stop_list = []\n",
    "stop_words_path = os.path.join(path, 'data\\\\中文停用词表.txt')\n",
    "for line in open(stop_words_path):\n",
    "    stop_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存放词频\n",
    "word_dict = {}\n",
    "word_list = []\n",
    "\n",
    "for filename in normal_dir:\n",
    "    data_path = os.path.join(path,'data\\\\normal')\n",
    "    file_name = os.path.join(data_path, filename)\n",
    "    #每个文档清空,word_list存放的是每个文档的唯一并且不在停词里面的词\n",
    "    word_list.clear()\n",
    "    \n",
    "    for line in open(file_name):\n",
    "        #正则表达式,去除非中文\n",
    "        relu = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "        line = relu.sub(\"\", line)\n",
    "        \n",
    "        #用jieba进行分词,中文分词\n",
    "        content = list(jieba.cut(line))\n",
    "        \n",
    "        #遍历每一个词\n",
    "        for word in content:\n",
    "            #没有停用词\n",
    "            if word not in stop_list and word.strip() != \"\" and word != None:\n",
    "                if word not in word_list:\n",
    "                    word_list.append(word)\n",
    "                    \n",
    "    #词典\n",
    "    #将每个文档中的词放在词典中，并计算所有文档中词出现的次数\n",
    "    for i in word_list:\n",
    "        #dict中出现过就加1，没出现过就默认为1\n",
    "        word_dict[i] = word_dict.get(i, 0)+1\n",
    "        \n",
    "normal_dict = word_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对spam数据的处理\n",
    "spam_path = os.path.join(path, 'data\\\\spam')\n",
    "word_dict.clear()\n",
    "\n",
    "for filename in spam_dir:\n",
    "    word_list.clear()\n",
    "    \n",
    "    #打开文件\n",
    "    f = open(os.path.join(spam_path, filename))\n",
    "    #获取文件每一行\n",
    "    for line in f:\n",
    "        #正则\n",
    "        relu = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "        line = relu.sub(\"\", line)\n",
    "        #用jieba进行分词\n",
    "        content = jieba.cut(line)\n",
    "        for s in content:\n",
    "            #进行判断\n",
    "            if s not in stop_list and s.strip()!='' and s!=None:\n",
    "                if s not in word_list:\n",
    "                    word_list.append(s)\n",
    "    for k in word_list:\n",
    "        word_dict[k] = word_dict.get(k, 0) +1\n",
    "spam_dict = word_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_len = len(normal_dir)\n",
    "spam_len = len(spam_dir)\n",
    "\n",
    "def get_test_words(test_dict, spam_dict, normal_dict, normal_len, spam_len):\n",
    "    word_prodict = {}\n",
    "    for word, num in test_dict.items():\n",
    "        if word in spam_dict.keys() and word in normal_dict.keys():\n",
    "            pw_s = spam_dict[word] / spam_len\n",
    "            pw_n = normal_dict[word]/normal_len\n",
    "            ps_w = pw_s/ (pw_s+pw_n)\n",
    "            word_prodict.setdefault(word, ps_w)\n",
    "\n",
    "        if word in spam_dict.keys() and word not in normal_dict.keys():\n",
    "            pw_s = spam_dict[word]/spam_len\n",
    "            pw_n = 0.01\n",
    "            ps_w = pw_s / (pw_s+pw_n)\n",
    "            word_prodict.setdefault(word, ps_w)\n",
    "\n",
    "        if word not in spam_dict.keys() and word in normal_dict.keys():\n",
    "            pw_s = 0.01\n",
    "            pw_n = normal_dict[word] / normal_len\n",
    "            ps_w = pw_s / (pw_s+pw_n)\n",
    "            word_prodict.setdefault(word, ps_w)\n",
    "\n",
    "        if word not in spam_dict.keys() and word not in normal_dict.keys():\n",
    "            word_prodict.setdefault(word, 0.47)\n",
    "            \n",
    "    sorted(word_prodict.items(), key=lambda d:d[1], reverse=True)[0:15]\n",
    "    return word_prodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calBayes(word_prolist, spam_dict, normal_dict):\n",
    "    ps_w = 1\n",
    "    ps_n = 1\n",
    "    for word, pro in word_prolist.items():\n",
    "        ps_w *= pro\n",
    "        ps_n *= (1-pro)\n",
    "    p = ps_w/ (ps_w+ps_n)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calAccuracy(test_result_dict):\n",
    "    r = 0\n",
    "    e = 0\n",
    "    for i, ic in test_result_dict.items():\n",
    "        if((int(i)<1000 and ic==0) or (int(i)>1000 and ic==1)):\n",
    "            r+=1\n",
    "        else:\n",
    "            e+=1\n",
    "    return(r/(r+e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率0.9464285714285714\n",
      "1/0\n",
      "10/0\n",
      "100/1\n",
      "101/0\n",
      "102/0\n",
      "103/0\n",
      "104/0\n",
      "105/0\n",
      "106/0\n",
      "107/0\n",
      "108/1\n",
      "109/0\n",
      "11/0\n",
      "110/0\n",
      "111/0\n",
      "112/0\n",
      "113/0\n",
      "114/0\n",
      "115/0\n",
      "116/0\n",
      "117/0\n",
      "118/0\n",
      "119/0\n",
      "120/0\n",
      "121/0\n",
      "122/0\n",
      "123/0\n",
      "124/0\n",
      "125/0\n",
      "126/0\n",
      "127/0\n",
      "128/0\n",
      "129/0\n",
      "13/0\n",
      "130/0\n",
      "131/0\n",
      "132/0\n",
      "133/0\n",
      "134/0\n",
      "135/0\n",
      "136/0\n",
      "137/0\n",
      "138/0\n",
      "139/0\n",
      "14/0\n",
      "140/0\n",
      "141/0\n",
      "142/0\n",
      "143/0\n",
      "144/0\n",
      "145/0\n",
      "146/0\n",
      "147/1\n",
      "148/0\n",
      "149/0\n",
      "15/0\n",
      "150/0\n",
      "151/0\n",
      "152/0\n",
      "153/0\n",
      "154/0\n",
      "155/0\n",
      "156/0\n",
      "157/0\n",
      "158/0\n",
      "159/0\n",
      "16/0\n",
      "160/0\n",
      "161/0\n",
      "162/0\n",
      "163/0\n",
      "164/0\n",
      "165/0\n",
      "166/0\n",
      "167/0\n",
      "168/0\n",
      "169/0\n",
      "17/0\n",
      "170/0\n",
      "171/0\n",
      "172/0\n",
      "173/0\n",
      "175/0\n",
      "176/0\n",
      "177/0\n",
      "178/0\n",
      "179/0\n",
      "18/0\n",
      "180/0\n",
      "181/0\n",
      "182/0\n",
      "183/0\n",
      "184/0\n",
      "185/0\n",
      "186/0\n",
      "187/0\n",
      "188/0\n",
      "189/0\n",
      "19/0\n",
      "190/1\n",
      "191/1\n",
      "192/0\n",
      "193/0\n",
      "194/0\n",
      "195/0\n",
      "196/0\n",
      "197/0\n",
      "198/0\n",
      "199/0\n",
      "2/0\n",
      "20/1\n",
      "200/0\n",
      "21/0\n",
      "22/0\n",
      "23/0\n",
      "24/0\n",
      "25/0\n",
      "26/0\n",
      "27/0\n",
      "28/0\n",
      "29/0\n",
      "3/0\n",
      "30/0\n",
      "31/0\n",
      "32/0\n",
      "33/0\n",
      "34/0\n",
      "35/0\n",
      "36/0\n",
      "37/0\n",
      "38/0\n",
      "39/0\n",
      "4/0\n",
      "40/0\n",
      "41/0\n",
      "42/0\n",
      "43/0\n",
      "44/0\n",
      "45/0\n",
      "46/0\n",
      "47/0\n",
      "48/0\n",
      "49/0\n",
      "5/0\n",
      "50/0\n",
      "51/0\n",
      "54/0\n",
      "55/0\n",
      "56/0\n",
      "57/0\n",
      "58/0\n",
      "59/1\n",
      "6/0\n",
      "60/0\n",
      "61/0\n",
      "62/0\n",
      "63/0\n",
      "65/0\n",
      "66/1\n",
      "67/0\n",
      "68/0\n",
      "69/0\n",
      "7/0\n",
      "70/0\n",
      "71/0\n",
      "72/0\n",
      "73/0\n",
      "74/0\n",
      "75/0\n",
      "76/0\n",
      "7801/1\n",
      "7802/1\n",
      "7803/0\n",
      "7804/1\n",
      "7805/1\n",
      "7806/1\n",
      "7807/1\n",
      "7808/1\n",
      "7809/1\n",
      "7810/1\n",
      "7811/1\n",
      "7812/1\n",
      "7813/1\n",
      "7814/1\n",
      "7815/1\n",
      "7816/1\n",
      "7817/1\n",
      "7818/0\n",
      "7819/1\n",
      "7820/1\n",
      "7821/1\n",
      "7822/1\n",
      "7823/1\n",
      "7824/1\n",
      "7825/1\n",
      "7826/1\n",
      "7827/1\n",
      "7828/1\n",
      "7829/1\n",
      "7830/1\n",
      "7831/1\n",
      "7832/1\n",
      "7833/1\n",
      "7834/1\n",
      "7835/1\n",
      "7836/1\n",
      "7837/1\n",
      "7838/1\n",
      "7839/1\n",
      "7840/1\n",
      "7841/1\n",
      "7842/0\n",
      "7843/1\n",
      "7844/1\n",
      "7845/1\n",
      "7846/1\n",
      "7847/1\n",
      "7848/1\n",
      "7849/1\n",
      "7850/1\n",
      "7851/1\n",
      "7852/1\n",
      "7853/1\n",
      "7854/1\n",
      "7855/1\n",
      "7856/1\n",
      "7857/1\n",
      "7858/1\n",
      "7859/1\n",
      "7860/1\n",
      "7861/1\n",
      "7862/1\n",
      "7863/1\n",
      "7864/1\n",
      "7865/1\n",
      "7866/1\n",
      "7867/1\n",
      "7868/1\n",
      "7869/1\n",
      "7870/1\n",
      "7871/1\n",
      "7872/1\n",
      "7873/1\n",
      "7874/1\n",
      "7875/1\n",
      "7876/1\n",
      "7877/1\n",
      "7878/1\n",
      "7879/1\n",
      "7880/1\n",
      "7881/1\n",
      "7882/1\n",
      "7883/1\n",
      "7884/0\n",
      "7885/1\n",
      "7886/1\n",
      "7887/1\n",
      "7888/1\n",
      "7889/1\n",
      "7890/1\n",
      "7891/1\n",
      "7892/1\n",
      "7893/1\n",
      "7894/1\n",
      "7895/1\n",
      "7896/1\n",
      "7897/1\n",
      "7898/1\n",
      "7899/1\n",
      "79/0\n",
      "7900/1\n",
      "7901/1\n",
      "7902/1\n",
      "7903/1\n",
      "7904/1\n",
      "7905/1\n",
      "7906/1\n",
      "7907/1\n",
      "7908/0\n",
      "7909/1\n",
      "7910/1\n",
      "7911/1\n",
      "7912/0\n",
      "7913/0\n",
      "7914/1\n",
      "7915/1\n",
      "7916/1\n",
      "7917/1\n",
      "7918/1\n",
      "7919/1\n",
      "7920/1\n",
      "7921/1\n",
      "7922/1\n",
      "7923/1\n",
      "7924/1\n",
      "7925/1\n",
      "7926/1\n",
      "7927/1\n",
      "7928/1\n",
      "7929/1\n",
      "7930/1\n",
      "7931/0\n",
      "7932/1\n",
      "7933/1\n",
      "7934/1\n",
      "7935/1\n",
      "7936/1\n",
      "7937/0\n",
      "7938/1\n",
      "7939/1\n",
      "7940/1\n",
      "7941/1\n",
      "7943/1\n",
      "7944/1\n",
      "7945/1\n",
      "7946/1\n",
      "7947/1\n",
      "7948/1\n",
      "7949/1\n",
      "7950/1\n",
      "7951/1\n",
      "7952/1\n",
      "7953/1\n",
      "7954/1\n",
      "7955/1\n",
      "7956/1\n",
      "7957/1\n",
      "7958/1\n",
      "7959/1\n",
      "7960/1\n",
      "7961/1\n",
      "7962/1\n",
      "7963/1\n",
      "7964/1\n",
      "7965/1\n",
      "7966/1\n",
      "7967/1\n",
      "7968/1\n",
      "7969/1\n",
      "7970/1\n",
      "7971/1\n",
      "7972/1\n",
      "7973/1\n",
      "7974/1\n",
      "7975/1\n",
      "7976/1\n",
      "7977/1\n",
      "7978/1\n",
      "7979/1\n",
      "7980/1\n",
      "7981/1\n",
      "7982/1\n",
      "7983/1\n",
      "7984/1\n",
      "7985/1\n",
      "7986/0\n",
      "7987/1\n",
      "7988/1\n",
      "7989/1\n",
      "7990/1\n",
      "7991/1\n",
      "7992/1\n",
      "7993/1\n",
      "7994/1\n",
      "7995/1\n",
      "7996/1\n",
      "7997/1\n",
      "7998/1\n",
      "7999/1\n",
      "8/0\n",
      "80/0\n",
      "8000/1\n",
      "81/1\n",
      "82/0\n",
      "83/0\n",
      "84/0\n",
      "85/0\n",
      "86/0\n",
      "87/0\n",
      "88/0\n",
      "89/1\n",
      "9/0\n",
      "90/0\n",
      "91/0\n",
      "92/0\n",
      "93/1\n",
      "94/0\n",
      "95/0\n",
      "96/0\n",
      "97/0\n",
      "98/0\n",
      "99/0\n"
     ]
    }
   ],
   "source": [
    "#对test数据的处理\n",
    "test_result_dict = {}\n",
    "test_path  = os.path.join(path, 'data\\\\test')\n",
    "for filename in test_dir:\n",
    "    test_file = os.path.join(test_path , filename)\n",
    "    \n",
    "    word_list.clear()\n",
    "    test_dict.clear()\n",
    "    word_dict.clear()\n",
    "    \n",
    "    f = open(test_file)\n",
    "    for line in f:\n",
    "        relu = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "        line = relu.sub(\"\", line)\n",
    "        content = jieba.cut(line)\n",
    "        for i in content:\n",
    "            if i not in stop_list and i.strip() != '' and i!=None:\n",
    "                if i not in word_list:\n",
    "                    word_list.append(i)\n",
    "    for t in word_list:\n",
    "        word_dict[t] = word_dict.get(t, 0)+1\n",
    "\n",
    "    test_dict = word_dict.copy() #这个文档的test_dict\n",
    "    \n",
    "    word_prolist = get_test_words(test_dict, spam_dict, normal_dict, normal_len, spam_len)\n",
    "   \n",
    "    \n",
    "    p = calBayes(word_prolist, spam_dict, normal_dict)\n",
    "    if p>0.9:\n",
    "        test_result_dict.setdefault(filename, 1)\n",
    "    else:\n",
    "        test_result_dict.setdefault(filename, 0)\n",
    "\n",
    "t = calAccuracy(test_result_dict)\n",
    "print(\"准确率\"+np.str(t))\n",
    "for i, ic in test_result_dict.items():\n",
    "    print(i+\"/\"+np.str(ic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
