{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "朴素贝叶斯算法，显然除了贝叶斯准备，朴素一词同样重要。这就是我们要说的条件独立性假设的概念。条件独立性假设是指特征之间的相互\n",
    "独立性假设，所谓独立，是指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "文档特征的方法。一种是词集模型，另外一种是词袋模型。顾名思义，词集模型就是对于一篇文档中出现的每个词，我们不考虑其\n",
    "出现的次数，而只考虑其在文档中是否出现，并将此作为特征；假设我们已经得到了所有文档中出现的词汇列表，那么根据每个词\n",
    "是否出现，就可以将文档转为一个与词汇列表等长的向量。而词袋模型，就是在词集模型的基础上，还要考虑单词在文档中出现的次数\n",
    "，从而考虑文档中某些单词出现多次所包含的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    '''\n",
    "    file_list中每一行表示每一个文档，并且每一行的大小不同\n",
    "    '''\n",
    "    file_list = [['my','dog','has','flea','problems','help','please'],\n",
    "                ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                ['stop','posting','stupid','worthless','garbage'],\n",
    "                ['my','licks','ate','my','steak','how','to','stop','him'],\n",
    "                ['quit','buying','worthless','dog','food','stupid']]\n",
    "    #类标签\n",
    "    file_labels = [0, 1, 0, 1, 0, 1]\n",
    "    return file_list, file_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocalList(file_list):\n",
    "    '''\n",
    "    求文档中的词条，即出现过的词\n",
    "    '''\n",
    "    vocal_list = set([])\n",
    "    for doc in file_list:\n",
    "        vocal_list = vocal_list|set(doc)  #求两个set集合的并集\n",
    "    return list(vocal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setword(vocal_list, inputset):\n",
    "    '''\n",
    "    判断文档中是否出现过该词，1指出现过，0没有出现过\n",
    "    '''\n",
    "    returnvec = len(vocal_list)*[0]\n",
    "    for vec in inputset:\n",
    "        if vec in vocal_list:\n",
    "            #出现过就是1\n",
    "            returnvec[vocal_list.index(vec)] = 1\n",
    "    return returnvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocal_list是词条列表\n",
    "file_list, file_labels = create_dataset()\n",
    "vocal_list = createVocalList(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词条向量，及文档中出现就是1，没有出现就是0,每个文档都有一个词条向量\n",
    "vec = setword(vocal_list, file_list[0])\n",
    "trainMatrix = []\n",
    "for i in range(len(file_list)):\n",
    "    vec = setword(vocal_list, file_list[i])\n",
    "    trainMatrix.append(vec)\n",
    "#每篇文档的vec\n",
    "#类标签，及file_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranNB(trainMatrix, trainCategory):\n",
    "    '''\n",
    "    trainMatrix表示一整个list中每个文档出现的的词条\n",
    "    trainCategory每一篇文档的类标签\n",
    "    \n",
    "    '''\n",
    "    #num_doc表示文档的数目，例如6\n",
    "    num_doc = len(trainMatrix)\n",
    "    #num_wor表示每个词条的长度，例如31\n",
    "    num_wor = len(trainMatrix[0])\n",
    "    #文档中类1所占的比例p(c=1)\n",
    "    pc = sum(trainCategory)/float(num_doc)\n",
    "    \n",
    "    p0_num = np.zeros(num_wor)\n",
    "    p1_num = np.zeros(num_wor)\n",
    "    p0demo = 0.0\n",
    "    p1demo = 0.0\n",
    "    \n",
    "    #遍历每一个文档\n",
    "    for i in range(num_doc):\n",
    "        #判断文档类别是否为1\n",
    "        if(trainCategory[i] == 1):\n",
    "            #类别为1的文档,放在p1_num里面\n",
    "            p1_num += trainMatrix[i]\n",
    "            #计算词条总数\n",
    "            p1demo += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0_num += trainMatrix[i]\n",
    "            p0demo += sum(trainMatrix[i])\n",
    "    #类别为1的条数 和 词的数目 的商\n",
    "    p1vect = p1_num / p1demo   \n",
    "    p0vect = p0_num / p0demo\n",
    "    return p0vect, p1vect, pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.04347826, 0.04347826, 0.04347826, 0.        ,\n",
       "        0.        , 0.13043478, 0.04347826, 0.        , 0.04347826,\n",
       "        0.04347826, 0.        , 0.        , 0.04347826, 0.04347826,\n",
       "        0.        , 0.04347826, 0.04347826, 0.08695652, 0.        ,\n",
       "        0.        , 0.        , 0.04347826, 0.04347826, 0.04347826,\n",
       "        0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.        ,\n",
       "        0.04347826]),\n",
       " array([0.10526316, 0.10526316, 0.        , 0.        , 0.05263158,\n",
       "        0.05263158, 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "        0.        , 0.05263158, 0.05263158, 0.        , 0.        ,\n",
       "        0.05263158, 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "        0.05263158, 0.15789474, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "        0.        ]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tranNB(trainMatrix, file_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vecClassify, p0vect, p1vect, pc):\n",
    "    '''\n",
    "    vecClassify:待分类的词条\n",
    "    p0vect:类别0的文档中词条出现的频数p(w0|c0)\n",
    "    p1vect:类别0的文档中词条出现的频数p(w1|c1)\n",
    "    pc:类别为1的文档的比例\n",
    "    '''\n",
    "    p1 = sum(vecClassify*p1vect) + math.log(pc)   #这是什么\n",
    "    p0 = sum(vecClassify*p0vect) + math.log(1.0 - pc)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNB():\n",
    "    #文档和标签\n",
    "    file_list, file_labels = create_dataset()\n",
    "    #词条\n",
    "    vocal_list = createVocalList(file_list)\n",
    "    #词条矩阵\n",
    "    train_mat = []\n",
    "    for doc in file_list:\n",
    "        doc_vocal = setword(vocal_list, doc)\n",
    "        train_mat.append(doc_vocal)\n",
    "    p0vect, p1vect, pc = tranNB(np.array(train_mat), np.array(file_labels))\n",
    "    \n",
    "    #测试文档\n",
    "    testfile = ['love','my','dalmation']\n",
    "    #将测试文档转为词条\n",
    "    test_doc_vocal = np.array(setword(vocal_list, testfile))\n",
    "    print(testfile, \"is\", classifyNB(test_doc_vocal,p0vect, p1vect, pc))\n",
    "    \n",
    "    #测试文档2\n",
    "    testfile2 = ['stupid','garbage']\n",
    "    test_doc_vocal2 = np.array(setword(vocal_list, testfile2))\n",
    "    print(testfile2,\"is\", classifyNB(test_doc_vocal2, p0vect, p1vect, pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] is 0\n",
      "['stupid', 'garbage'] is 1\n"
     ]
    }
   ],
   "source": [
    "testNB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
